import os
import json
import logging
import asyncio
import random
from datetime import datetime
from playwright.async_api import async_playwright

logger = logging.getLogger("mcp_vision_server.archival_scraper")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

COOKIES_FILE = "cookies.json"
HISTORY_FILE = "scraped_history.json"
MASTER_LINKS_FILE = "master_saved_links.json"
DOWNLOAD_DIR = "./watched_files/instagram"

async def ensure_download_dir():
    if not os.path.exists(DOWNLOAD_DIR):
        os.makedirs(DOWNLOAD_DIR)

async def harvest_all_links(page) -> list[str]:
    """ë¸Œë¼ìš°ì €ì˜ JS ì—”ì§„ì— ì§ì ‘ ì¹¨íˆ¬í•˜ì—¬ ê°€ìƒ DOM ì†Œë©¸ í˜„ìƒì„ ìš°íšŒí•˜ê³  ëª¨ë“  URLì„ ìˆ˜ê±°í•©ë‹ˆë‹¤."""
    logger.info("=====================================================")
    logger.info("ğŸš€ [1ë‹¨ê³„] ì´ˆê³ ì† ë”¥ ìŠ¤ìº” í•˜ë² ìŠ¤íŒ…(Harvesting) ê°€ë™")
    logger.info("=====================================================")
    logger.info("ì¸ìŠ¤íƒ€ê·¸ë¨ í™”ë©´ í•˜ë‹¨ìœ¼ë¡œ ë¬´í•œ ê°•í•˜í•˜ë©° 3ë…„ ì¹˜ ë§í¬ë¥¼ ìºì‹±í•©ë‹ˆë‹¤. (ìµœëŒ€ ìˆ˜ ë¶„ ì†Œìš”)")
    
    js_script = """
    async () => {
        return new Promise((resolve) => {
            const collectedLinks = new Set();
            let lastScrollHeight = 0;
            let unchangedScrollCount = 0;
            
            const extractLinks = () => {
                const links = document.querySelectorAll('a[href*="/p/"]', 'a[href*="/reel/"]');
                links.forEach(a => {
                    const href = a.getAttribute('href');
                    if (href.includes('/p/') || href.includes('/reel/')) {
                        collectedLinks.add(href);
                    }
                });
            };

            const scrollInterval = setInterval(() => {
                extractLinks();
                window.scrollTo(0, document.body.scrollHeight);
                
                if (document.body.scrollHeight === lastScrollHeight) {
                    unchangedScrollCount++;
                    if (unchangedScrollCount > 8) { // ì•½ 10~15ì´ˆê°„ ë” ì´ìƒ í˜ì´ì§€ê°€ ì•ˆ ëŠ˜ì–´ë‚˜ë©´ ë°”ë‹¥ì— ë„ë‹¬í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        clearInterval(scrollInterval);
                        resolve(Array.from(collectedLinks));
                    }
                } else {
                    lastScrollHeight = document.body.scrollHeight;
                    unchangedScrollCount = 0;
                }
            }, 1200); // 1.2ì´ˆë§ˆë‹¤ í•˜ê°• ë° ìŠ¤ìº”
            
            // ì²« í™”ë©´ ìŠ¤ìº”
            extractLinks();
        });
    }
    """
    links = await page.evaluate(js_script)
    logger.info(f"ğŸ¯ í•˜ë² ìŠ¤íŒ… ì™„ë£Œ! ì´ {len(links)}ê°œì˜ ê³ ìœ í•œ í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ íšë“í–ˆìŠµë‹ˆë‹¤.")
    return links

async def run_archival_dump():
    if not os.path.exists(COOKIES_FILE):
        logger.error(f"'{COOKIES_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    await ensure_download_dir()
    
    processed_hrefs = set()
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                processed_hrefs = set(json.load(f))
        except:
            pass
            
    try:
        master_links = []
        if os.path.exists(MASTER_LINKS_FILE):
            logger.info("ğŸ“¦ ë¡œì»¬ ìºì‹œì—ì„œ ê¸°ì¡´ ë§ˆìŠ¤í„° ë§í¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ìƒˆ ë°ì´í„°ë¥¼ ì›í•˜ë©´ ì´ íŒŒì¼ì„ ì§€ìš°ì„¸ìš”)")
            with open(MASTER_LINKS_FILE, "r", encoding="utf-8") as f:
                master_links = json.load(f)
        else:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
                context = await browser.new_context(viewport={'width': 1280, 'height': 800}, user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                with open(COOKIES_FILE, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                await context.add_cookies(cookies)
                page = await context.new_page()
                
                logger.info("ì•ˆì „í•˜ê³  ì€ë°€í•˜ê²Œ ì¸ìŠ¤íƒ€ê·¸ë¨ ë³¸ì§„ì— ì§„ì…í•©ë‹ˆë‹¤...")
                await page.goto("https://www.instagram.com/", wait_until="domcontentloaded")
                await asyncio.sleep(3)
                
                profile_link_element = await page.wait_for_selector('a[href^="/"]:has(img)', timeout=15000)
                if not profile_link_element:
                    logger.error("ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¿ í‚¤ë¥¼ ë‹¤ì‹œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.")
                    await browser.close()
                    return
                    
                profile_href = await profile_link_element.get_attribute("href")
                saved_url = f"https://www.instagram.com{profile_href}saved/all-posts/"
                
                logger.info(f"ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ë¡œ ì§í–‰: {saved_url}")
                await page.goto(saved_url, wait_until="domcontentloaded")
                await asyncio.sleep(4)
                
                master_links = await harvest_all_links(page)
                with open(MASTER_LINKS_FILE, "w", encoding="utf-8") as f:
                    json.dump(master_links, f)
                await browser.close()

        links_to_process = [link for link in master_links if link not in processed_hrefs]
        logger.info(f"ğŸ”¥ ì´ {len(links_to_process)}ê°œì˜ ìƒˆ ìë£Œë¥¼ ë‹¤ìš´ë¡œë“œ íì— ë“±ë¡í–ˆìŠµë‹ˆë‹¤.")
        if len(links_to_process) == 0:
            logger.info("ë” ì´ìƒ ë‹¤ìš´ë¡œë“œí•  ìƒˆë¡œìš´ ì•„ì¹´ì´ë¸Œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info("=====================================================")
        logger.info("ğŸš€ [2ë‹¨ê³„] ê°œë³„ ê²Œì‹œë¬¼ ë…ë¦½ ë‹¤ìš´ë¡œë“œ (Direct Extraction) - GC ëª¨ë“œ")
        logger.info("=====================================================")
        
        CHUNK_SIZE = 500
        total_processed = 0

        for chunk_idx in range(0, len(links_to_process), CHUNK_SIZE):
            chunk = links_to_process[chunk_idx:chunk_idx + CHUNK_SIZE]
            logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜(GC): ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ë¥¼ (ì¬)ì‹œì‘í•©ë‹ˆë‹¤. ì˜ˆìƒ RAM í™•ë³´ [Chunk {chunk_idx//CHUNK_SIZE + 1}]")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled'])
                context = await browser.new_context(viewport={'width': 1280, 'height': 800}, user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                with open(COOKIES_FILE, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                await context.add_cookies(cookies)
                page = await context.new_page()

                for i, link in enumerate(chunk, 1):
                    total_processed += 1
                    logger.info(f"[{total_processed}/{len(links_to_process)}] ì¶”ì¶œ ì¤‘: {link}")
                    try:
                        target_url = f"https://www.instagram.com{link}"
                        await page.goto(target_url, wait_until="domcontentloaded")
                        await asyncio.sleep(random.uniform(0.7, 1.2))

                        shortcode = link.strip("/").split("/")[-1]
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename_base = f"ig_{shortcode}_{timestamp}"

                        # ê²Œì‹œë¬¼ DOM ë³€ê²½ í˜¹ì€ Reel ëŒ€ì‘ì„ ìœ„í•´ fallback ì¶”ê°€
                        article = page.locator('article')
                        if await article.count() == 0:
                            article = page.locator('main[role="main"]')
                        if await article.count() == 0:
                            article = page.locator('body')
                            
                        article = article.first
                        await article.wait_for(state="visible", timeout=12000)

                        post_text = await article.inner_text()
                        txt_path = os.path.join(DOWNLOAD_DIR, f"{filename_base}.txt")
                        with open(txt_path, "w", encoding="utf-8") as f:
                            f.write(post_text)

                        carousel_idx = 0
                        while True:
                            img_path = os.path.join(DOWNLOAD_DIR, f"{filename_base}_{carousel_idx}.png")
                            await article.screenshot(path=img_path)

                            next_btn = article.locator('button[aria-label="Next"]')
                            if await next_btn.count() > 0:
                                await next_btn.click()
                                await asyncio.sleep(0.4)
                                carousel_idx += 1
                            else:
                                break

                        processed_hrefs.add(link)
                        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                            json.dump(list(processed_hrefs), f)

                        sleep_time = random.uniform(1.5, 3.0)
                        logger.info(f"  -> ì™„ë£Œ (ìŠ¬ë¼ì´ë“œ {carousel_idx + 1}ì¥). ì‚¬ëŒì²˜ëŸ¼ {sleep_time:.1f}ì´ˆ íœ´ì‹í•©ë‹ˆë‹¤...")
                        await asyncio.sleep(sleep_time)

                    except Exception as e:
                        logger.warning(f"  -> ì—‘ì„¸ìŠ¤ ì—ëŸ¬ (ê±´ë„ˆëœ€): {link} - {e}")
                        await asyncio.sleep(2)
                
                await browser.close()
                logger.info("ğŸ§¹ ì²­í¬ ë‹¬ì„± ì™„ë£Œ. ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ìŠµë‹ˆë‹¤.")
                await asyncio.sleep(3)

        logger.info("ğŸ‰ 3ë…„ ì¹˜ ì•„ì¹´ì´ë¸Œ ì „ì²´ ê°•ì œ ë¤í”„ í”„ë¡œì„¸ìŠ¤ê°€ ë¬´ì‚¬íˆ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(run_archival_dump())
